---
layout: post
title: "Simhash原理"
description: "Simhash"
category: Machine Learning


---
{% include JB/setup %}


<script type='text/javascript' src='http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

###什么是Simhash

Simhash是一种LSH(Local sensitive hash)，也就是说如果两个hash前的向量他们的夹角是相近的，hash之后得到的simhash值汉明距离是接近的。

关于怎么做simhash，这张图说的已经十分明白了。

<img src="http://dl.iteye.com/upload/attachment/437426/baf42378-e625-35d2-9a89-471524a355d8.jpg" alt="Simhash" style="width: 500px;"/>

简单的说，就是首先对每个feature生成它们的hash值（01串）（hash函数没有特别的要求，均匀就可以），然后hash值为1的，将1变成weight，为0的，将0变成-weight，生成的这个feature的hashcode。最后把所有feature的hashcode相加，正值就是1，反之为0，就生成完毕了。

###怎么用？

这个不多说，生成出simhash之后，比较汉明距离就可以了，汉明距离在一定范围内的，可以认为是相同或者相似的向量。

###Simhash的原理（自己扯淡的）

Google + 百度了一圈，综合了一下说一下自己的看法。

看上图，我们之前是把每个feature映射成了一个低维的hash值（假设32位），现在我们竖着看这些向量，总共有32个n维向量，记做$$(h_1,h_2,...,h_{32})$$。

将这个向量的权重记为w，则它的$$simhash(x) = sign(wh_1, wh_2,..., wh_{32})$$，这个代表什么意思呢，可以这么理解，每个$$h_i$$都代表着一个n维的超平面，$$sign(wh_i)$$为正，代表w在$$h_i$$的上面，为负则代表在下面。

下面考虑两个向量$$w_1,w_2$$，假设他俩夹角为$$\theta$$，则随机搞一个超平面，他俩分属超平面两端（也就是$$sign(w_1h_i) != sign(w_2h_i)$$）的概率为$$\frac{1-\theta}{2\pi}$$，这是与夹角大小副相关的，即如果这两个向量夹角越小，他们simcode第i位不同的概率就越小。他们的simcode不同的位数也越少，到这里，之前要求的hash函数均匀也就有道理了。


