---
layout: post
title: "Simhash原理"
description: "Simhash"
category: Machine Learning


---
{% include JB/setup %}


<script type='text/javascript' src='http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

###什么是Simhash

Simhash是一种LSH(Local sensitive hash)，也就是说如果两个hash前的向量他们的夹角是相近的，hash之后得到的simhash值汉明距离是接近的。

关于怎么做simhash，这张图说的已经十分明白了。

<img src="http://dl.iteye.com/upload/attachment/437426/baf42378-e625-35d2-9a89-471524a355d8.jpg" alt="Simhash" style="width: 500px;"/>

简单的说，就是首先对每个feature生成它们的hash值（01串）（hash函数没有特别的要求，均匀就可以），然后hash值为1的，将1变成weight，为0的，将0变成-weight，生成的这个feature的hashcode。最后把所有feature的hashcode相加，正值就是1，反之为0，就生成完毕了。

###怎么用？

这个不多说，生成出simhash之后，比较汉明距离就可以了，汉明距离在一定范围内的，可以认为是相同或者相似的向量。

###Simhash的原理（自己扯淡的）

Google + 百度了一圈，综合了一下说一下自己的看法。

看上图，我们之前是把每个feature映射成了一个低维的hash值（假设32位），现在我们竖着看这些向量，总共有32个n维向量，记做$$(h_1,h_2,...,h_{32})$$


