---
layout: post
title: "Logistic Regression的贝叶斯先验，后验"
description: "Logistic Regression"
category: Machine Learning


---
{% include JB/setup %}


<script type='text/javascript' src='http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

面试今日头条的时候，问到了这个问题，在LR中，L1与L2norm分别对先验概率做出了怎样的假设。当时我没答上来，说自己对贝叶斯没啥研究，回来看了一下参考文献，应该是想通了这个问题。也更加的认识到LR是一个很虎的模型，可以根据LR学习到很多东西。

###Logistic Regression的优化函数

关于LR的模型不再赘述，LR的目标无非就是要求出参数$$w$$，使得这个$$w$$是最好的。

基本上所有教材都是直接上似然函数，要最大化似然函数来求出$$w$$，也就是$$max(P(Y\|X,w))$$，但是似然函数只是贝叶斯展开的一个组成部分而已，注意到，我们要找到最好的$$w$$，那么什么是最好的$$w$$呢？

就是给定$$X,Y$$使得$$P(w\|X,Y)$$最大，我们用贝叶斯公式将其展开得到$$P(w\|X,Y) = \frac{P(Y\|X,w)*P(w\|X)*P(X)}{P(X,Y)}$$，可以看到对于所有的$$w$$，$$P(X),P(X,Y)$$都是一样的，而$$X,w$$相互独立，所以我们需要最大化的公式变成了$$P(w\|X,Y) \propto P(Y\|X,w)*P(w)$$，我们要最大化的，其实上是个后验概率（由先验概率($$p(w)$$)与似然函数($$P(Y\|X,w)$$)组成），这个方法叫MAP(maximize a posteriori)最大后验概率估计，与MLE(maximize likelihood estimation)最大似然估计是类似的，区别就在于多了一个先验概率。

###LR的MAP估计（最大后验概率）

好了，现在来看一下使用MAP的LR。





